{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ecfbb4b0c704e67",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Web Scraping and Introductory Data Analysis\n",
    "\n",
    "Welcome to Homework 0, where we will delve into web scraping and perform an introductory data analysis. This homework will be a hands-on exercise that will help you become familiar with the process of extracting data from websites and conducting basic statistical analysis. \n",
    "\n",
    "## Objectives\n",
    "\n",
    "By the end of this homework, you will be able to:\n",
    "\n",
    "1. Set up a Python environment with the necessary libraries for web scraping and data analysis.\n",
    "2. Write a web scraping script using Beautiful Soup and Selenium to collect data from a website.\n",
    "3. Sample from the collected dataset and compare the statistics of the sample and the population.\n",
    "   \n",
    "## Tasks\n",
    "\n",
    "1. **Environment Setup**: Install the required libraries such as Beautiful Soup, Selenium, pandas, numpy, matplotlib, and seaborn.\n",
    "\n",
    "2. **Web Scraping**: Write a script to scrape transaction data from [Etherscan.io](https://etherscan.io/txs). Use Selenium to interact with the website and Beautiful Soup to parse the HTML content.\n",
    "\n",
    "3. **Data Sampling**: Once the data is collected, create a sample from the dataset. Compare the sample statistics (mean and standard deviation) with the population statistics.\n",
    "\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "1. A Jupyter notebook with all the code and explanations.\n",
    "2. A detailed report on the findings, including the comparison of sample and population statistics.\n",
    "Note: You can include the report in your notebook.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "Begin by setting up your Python environment and installing the necessary libraries. Then, proceed with the web scraping task, ensuring that you handle any potential issues such as rate limiting. Once you have the data, move on to the data sampling and statistical analysis tasks. \n",
    "\n",
    "Remember to document your process and findings in the Jupyter notebook, and to include visualizations where appropriate to illustrate your results. <br>\n",
    "Good luck, and happy scraping!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca352a49724d191",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data Collection (Etherscan)\n",
    "\n",
    "In this section, we will use web scraping to gather transaction data from the Ethereum blockchain using the Etherscan block explorer. Our objective is to collect transactions from the **last 10 blocks** on Ethereum.\n",
    "\n",
    "To accomplish this task, we will employ web scraping techniques to extract the transaction data from the Etherscan website. The URL we will be targeting for our data collection is:\n",
    "\n",
    "[https://etherscan.io/txs](https://etherscan.io/txs)\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Navigate to the URL**: Use Selenium to open the Etherscan transactions page in a browser.\n",
    "\n",
    "2. **Locate the Transaction Data**: Identify the HTML elements that contain the transaction data for the specified block range.\n",
    "\n",
    "3. **Extract the Data**: Write a script to extract the transaction details e.g. Hash, Method, Block, etc.\n",
    "\n",
    "4. **Handle Pagination**: If the transactions span multiple pages, implement pagination handling to navigate through the pages and collect all relevant transaction data.\n",
    "\n",
    "5. **Store the Data**: Save the extracted transaction data into a structured format, such as a CSV file or a pandas DataFrame, for further analysis.\n",
    "\n",
    "### Considerations\n",
    "\n",
    "- **Rate Limiting**: Be mindful of the website's rate limits to avoid being blocked. Implement delays between requests if necessary.\n",
    "- **Dynamic Content**: The Etherscan website may load content dynamically. Ensure that Selenium waits for the necessary elements to load before attempting to scrape the data.\n",
    "- **Data Cleaning**: After extraction, clean the data to remove any inconsistencies or errors that may have occurred during the scraping process.\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Selenium Documentation](https://selenium-python.readthedocs.io/)\n",
    "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- [Ethereum](https://ethereum.org/en/)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install & import necessary Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb0ca63c8ac07aba"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "# Check if Selenium is installed\n",
    "if importlib.util.find_spec(\"selenium\") is None:\n",
    "    # Install Selenium\n",
    "    !pip install selenium\n",
    "\n",
    "# Check if BeautifulSoup (bs4) is installed\n",
    "if importlib.util.find_spec(\"bs4\") is None:\n",
    "    # Install BeautifulSoup (bs4)\n",
    "    !pip install beautifulsoup4\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "# from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "import time\n",
    "import random"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T10:50:42.179347600Z",
     "start_time": "2024-03-09T10:50:42.063574400Z"
    }
   },
   "id": "c7265854-2bda-41b3-b0ac-ca254496af79",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Scraping"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "663b49c83972f414"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "54fa10db-ec9e-4921-870a-50066926ed2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T00:57:41.322105900Z",
     "start_time": "2024-03-09T00:52:35.742184100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Navigation to etherscan URL!\n",
      "SUCCESS: Latest block number is acquired!\n",
      "SUCCESS: Navigation to block number 1's URL!\n",
      "SUCCESS: Data of block number 1 is scraped!\n",
      "SUCCESS: Navigation to block number 2's URL!\n",
      "SUCCESS: Data of block number 2 is scraped!\n",
      "SUCCESS: Navigation to block number 3's URL!\n",
      "SUCCESS: Data of block number 3 is scraped!\n",
      "SUCCESS: Navigation to block number 4's URL!\n",
      "SUCCESS: Data of block number 4 is scraped!\n",
      "SUCCESS: Navigation to block number 5's URL!\n",
      "SUCCESS: Data of block number 5 is scraped!\n",
      "SUCCESS: Navigation to block number 6's URL!\n",
      "SUCCESS: Data of block number 6 is scraped!\n",
      "SUCCESS: Navigation to block number 7's URL!\n",
      "SUCCESS: Data of block number 7 is scraped!\n",
      "SUCCESS: Navigation to block number 8's URL!\n",
      "SUCCESS: Data of block number 8 is scraped!\n",
      "SUCCESS: Navigation to block number 9's URL!\n",
      "SUCCESS: Data of block number 9 is scraped!\n",
      "SUCCESS: Navigation to block number 10's URL!\n",
      "SUCCESS: Data of block number 10 is scraped!\n",
      "SUCCESS: Raw scraped data is saved!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Navigate to the main page URL\n",
    "\n",
    "domain = \"https://etherscan.io\"\n",
    "homepage_url = domain + \"/txs\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(homepage_url)\n",
    "\n",
    "print(\"SUCCESS: Navigation to etherscan URL!\")\n",
    "\n",
    "# Step 2: Locate the Transaction Data\n",
    "\n",
    "wait = WebDriverWait(driver, 10)    # Wait for the table to load (dynamic content consideration)\n",
    "wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'table')))\n",
    "\n",
    "# Extract the latest transaction's block number\n",
    "transaction_table = driver.find_element(By.CLASS_NAME, 'table')\n",
    "latest_transaction = transaction_table.find_element(By.XPATH, \"//tbody/tr[1]\")\n",
    "latest_block_number = latest_transaction.find_element(By.XPATH, \"./td[4]/a\").text\n",
    "\n",
    "print(\"SUCCESS: Latest block number is acquired!\\n\")\n",
    "\n",
    "# Set the number of latest blocks needed to be scraped\n",
    "target_block_count = 10\n",
    "transaction_data = []\n",
    "\n",
    "# Step 3: Extract the Data\n",
    "# Step 4: Handle Pagination\n",
    "\n",
    "for counter in range(target_block_count):\n",
    "    # Navigate to the URL that a block's data is recorded\n",
    "    block_number = int(latest_block_number) - counter\n",
    "    query_string = \"?block=\" + str(block_number)\n",
    "    current_block_url = homepage_url + query_string\n",
    "    driver.get(current_block_url)\n",
    "    \n",
    "    print(\"SUCCESS: Navigation to block number \" + str(counter + 1) + \"'s URL!\")\n",
    "\n",
    "    while True:    # Continue scraping data until all block data is scraped from multiple pages\n",
    "        wait = WebDriverWait(driver, 10)    # Wait for the table to load (dynamic content consideration)\n",
    "        wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'table')))\n",
    "    \n",
    "        # Locate the transaction data table and its rows\n",
    "        table = driver.find_element(By.CLASS_NAME, 'table')\n",
    "        rows = table.find_elements(By.XPATH, \"//tbody/tr\")\n",
    "\n",
    "        for row in rows:    # Extracting relevant columns using xpath of elements containing the actual data\n",
    "            Txn_hash = row.find_element(By.XPATH, \"./td[2]/div/span/a\").text\n",
    "            Method = row.find_element(By.XPATH, \"./td[3]/span\").text\n",
    "            Block = row.find_element(By.XPATH, \"./td[4]/a\").text\n",
    "            Age = row.find_element(By.XPATH, \"./td[6]/span\").text\n",
    "            From = row.find_element(By.XPATH, \"./td[8]/div/a\").text\n",
    "            To = row.find_element(By.XPATH, \"./td[10]/div/a[1]\").text\n",
    "            Value = row.find_element(By.XPATH, \"./td[11]/span\").text\n",
    "            Txn_Fee = row.find_element(By.XPATH, \"./td[12]\").text\n",
    "            \n",
    "            # Append the data to the database\n",
    "            transaction_data.append({'Txn_Hash':Txn_hash, 'Method':Method, 'Block':Block, 'Age':Age, 'From':From, 'To':To, 'Value':Value, 'Txn_Fee': Txn_Fee})\n",
    "    \n",
    "        # Navigate to next page if there is one\n",
    "        pagination_panel = driver.find_element(By.CLASS_NAME, 'pagination')\n",
    "        if \"disabled\" not in pagination_panel.find_element(By.XPATH, \".//li[4]\").get_attribute(\"class\"):\n",
    "            next_page_button = pagination_panel.find_element(By.XPATH, \".//li[4]/a\")\n",
    "            time.sleep(random.uniform(1, 3))    # Add a random delay between 1 and 3 seconds to consider rate limiting\n",
    "            next_page_button.click()\n",
    "        else:\n",
    "            break   # All of the block's data is scraped. Move on to the next block.\n",
    "            \n",
    "        time.sleep(random.uniform(1, 3))    # Add a random delay between 1 and 3 seconds to consider rate limiting\n",
    "    \n",
    "    print(\"SUCCESS: Data of block number \" + str(counter + 1) + \" is scraped!\")\n",
    "    \n",
    "# Step 5: Store the Data\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "df = pd.DataFrame(transaction_data)\n",
    "# Save the raw DataFrame to a CSV file\n",
    "df.to_csv('raw_ethereum_transactions.csv', index=False)\n",
    "\n",
    "print(\"\\nSUCCESS: Raw scraped data is saved!\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cba2c07f13222bf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data Analysis\n",
    "\n",
    "Now that we have collected the transaction data from Etherscan, the next step is to perform conduct an initial analysis. This task will involve the following steps:\n",
    "\n",
    "1. **Load the Data**: Import the collected transaction data into a pandas DataFrame.\n",
    "\n",
    "2. **Data Cleaning**: Clean the data by converting data types, removing any irrelevant information, and handling **duplicate** values.\n",
    "\n",
    "3. **Statistical Analysis**: Calculate the mean and standard deviation of the population. Evaluate these statistics to understand the distribution of transaction values. The analysis and plotting will be on **Txn Fee** and **Value**.\n",
    "\n",
    "4. **Visualization**: This phase involves the creation of visual representations to aid in the analysis of transaction values. The visualizations include:\n",
    "    - A histogram for each data column, which provides a visual representation of the data distribution. The selection of bin size is crucial and should be based on the data's characteristics to ensure accurate representation. Provide an explanation on the bin size selection!\n",
    "    - A normal distribution plot fitted alongside the histogram to compare the empirical distribution of the data with the theoretical normal distribution.\n",
    "    - A box plot and a violin plot to identify outliers and provide a comprehensive view of the data's distribution.\n",
    "\n",
    "### Deliverables\n",
    "\n",
    "The project aims to deliver the following deliverables:\n",
    "\n",
    "- A refined pandas DataFrame containing the transaction data, which has undergone thorough cleaning and is ready for analysis.\n",
    "- A simple statistical analysis evaluating the population statistics, offering insights into the distribution of transaction values and fees.\n",
    "- A set of visualizations showcasing the distribution of transaction values for the population. These visualizations include histograms, normal distribution plots, box plots, and violin plots, each serving a specific purpose in the analysis.\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "The project starts with the importing of transaction data into a pandas DataFrame, setting the stage for data manipulation and analysis. Subsequent steps involve the cleaning of the data to ensure its quality and reliability. Followed by the calculation of population statistics. Finally, a series of visualizations are created to visually analyze the distribution of transaction values and fees."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install & import necessary libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "963634397e43f0b1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T10:50:50.042554500Z",
     "start_time": "2024-03-09T10:50:50.035039100Z"
    }
   },
   "id": "f74a27ed4f040f45",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Cleaning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bd0adc46046f5ce"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: 0.0\n",
      "Q3: 0.0512833435\n",
      "IQR: 0.0512833435\n",
      "Max Lower Whisker Reach: -0.07692501525\n",
      "Max Upper Whisker Reach: 0.12820835875\n",
      "SUCCESS: Clean data is saved!\n"
     ]
    }
   ],
   "source": [
    "# Function to convert wei (a unit of Ethereum cryptocurrency) to ether\n",
    "def wei_to_eth(wei):\n",
    "    return wei / 10**18\n",
    "  \n",
    "# Step 1: Load the Data\n",
    "    \n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('raw_ethereum_transactions.csv')\n",
    "\n",
    "# Step 2: Data Cleaning\n",
    "\n",
    "# Remove duplicate rows from the DataFrame\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Drop the useless columns from the DataFrame\n",
    "df = df.drop(['Txn_Hash', 'Block', 'Age', 'From', 'To'], axis=1)\n",
    "\n",
    "# Fill missing values with a specific value\n",
    "df.fillna({'Value': 0, 'Txn_Fee': 0, 'Method': 'other'}, inplace=True)\n",
    "\n",
    "# Create a new column 'Method_Category' with default value 'other'\n",
    "df['Method_Category'] = 'Other'\n",
    "\n",
    "# Define conditions for each category and update the 'Method_Category' accordingly\n",
    "df.loc[df['Method'].str.lower().str.startswith('transfer'), 'Method_Category'] = 'Transfer'\n",
    "df.loc[df['Method'].str.lower().str.startswith('0x'), 'Method_Category'] = 'Unidentified'\n",
    "df.loc[df['Method'].str.lower().str.startswith('approve'), 'Method_Category'] = 'Approve'\n",
    "df.loc[df['Method'].str.lower().str.startswith('execute'), 'Method_Category'] = 'Execute'\n",
    "df.loc[df['Method'].str.lower().str.startswith('swap'), 'Method_Category'] = 'Swap'\n",
    "\n",
    "# Drop the original 'Method' column since it's not needed anymore\n",
    "df = df.drop('Method', axis=1)\n",
    "\n",
    "# Rename the new 'Method_Category' column to 'Method'\n",
    "df.rename(columns={'Method_Category': 'Method'}, inplace=True)\n",
    "\n",
    "# Convert wei values to eth values\n",
    "df['Value'] = df['Value'].apply(lambda x: wei_to_eth(int(x.split()[0])) if 'wei' in x else x)\n",
    "\n",
    "# Convert Value & Txn_Fee columns to a numeric type\n",
    "df['Value'] = df['Value'].str.replace('ETH' , '')   # Remove ETH from the value string\n",
    "df['Value'] = pd.to_numeric(df['Value'], errors='coerce')\n",
    "df['Txn_Fee'] = pd.to_numeric(df['Txn_Fee'], errors='coerce')\n",
    "\n",
    "# Make a copy of the DataFrame and drop transactions with 'Value' == 0 and save to a CSV file\n",
    "df_without_zeros = df.copy()\n",
    "df_without_zeros = df_without_zeros.drop(df_without_zeros[df_without_zeros['Value'] == 0].index)\n",
    "df_without_zeros.to_csv('clean_ethereum_transactions_without_zeros.csv', index=False)\n",
    "\n",
    "# Calculate the interquartile range (IQR) for Value column\n",
    "Q1 = df['Value'].quantile(0.25)\n",
    "Q3 = df['Value'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Calculate max upper & lower whisker reach\n",
    "# Max_Lower_Whisker_Reach = Q1 - 1.5 * IQR\n",
    "# Max_Upper_Whisker_Reach = Q3 + 1.5 * IQR\n",
    "# \n",
    "# print(\"Q1:\", Q1)\n",
    "# print(\"Q3:\", Q3)\n",
    "# print(\"IQR:\", IQR)\n",
    "# print(\"Max Lower Whisker Reach:\", Max_Lower_Whisker_Reach)\n",
    "# print(\"Max Upper Whisker Reach:\", Max_Upper_Whisker_Reach)\n",
    "\n",
    "# Save the clean DataFrame to a CSV file\n",
    "df.to_csv('clean_ethereum_transactions.csv', index=False)\n",
    "\n",
    "print(\"SUCCESS: Clean data is saved!\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T10:50:53.966615500Z",
     "start_time": "2024-03-09T10:50:53.875401900Z"
    }
   },
   "id": "f481b11a08d876b6",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccdb960b48f1c7b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Statistical Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78344bed669a4cba"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------\n",
      "                            Ungrouped Statistical Analysis                              \n",
      "----------------------------------------------------------------------------------------\n",
      "                                       Min         Max      Mean    Median  \\\n",
      "Full Dataset with 0: Value    0.000000e+00  390.001242  0.598981  0.000000   \n",
      "Excluding 0 Values:  Value    1.000000e-09  390.001242  1.254723  0.058440   \n",
      "Full Dataset with 0: Txn_Fee  9.229100e-04    0.257939  0.004615  0.002119   \n",
      "\n",
      "                              Standard Deviation   Count  \n",
      "Full Dataset with 0: Value              9.831192  1680.0  \n",
      "Excluding 0 Values:  Value             14.204649   802.0  \n",
      "Full Dataset with 0: Txn_Fee            0.010096  1683.0  \n",
      "\n",
      "SUCCESS: Ungrouped Statistical Analysis conducted!\n",
      "----------------------------------------------------------------------------------------\n",
      "                             Grouped Statistical Analysis                               \n",
      "----------------------------------------------------------------------------------------\n",
      "--> Grouped analysis for Value (Full Dataset with 0):\n",
      "\n",
      "                  Mean    Median  Standard Deviation  Count\n",
      "Method                                                     \n",
      "Approve       0.000000  0.000000            0.000000    140\n",
      "Execute       0.305751  0.016197            0.714321    159\n",
      "Other         2.064265  0.000000           24.434047    263\n",
      "Swap          0.320887  0.000000            0.990983     51\n",
      "Transfer      0.402516  0.007000            2.304944    944\n",
      "Unidentified  0.149847  0.000000            0.460847    123\n",
      "----------------------------------------------------------------------------------------\n",
      "--> Grouped analysis for Value (Excluding 0 Values):\n",
      "\n",
      "                  Mean    Median  Standard Deviation  Count\n",
      "Method                                                     \n",
      "Execute       0.578743  0.196000            0.900796     84\n",
      "Other         6.100020  0.106542           41.864082     89\n",
      "Swap          0.681886  0.182637            1.370410     24\n",
      "Transfer      0.682181  0.044785            2.969773    557\n",
      "Unidentified  0.383982  0.045663            0.677841     48\n",
      "----------------------------------------------------------------------------------------\n",
      "--> Grouped analysis for Txn_Fee (Full Dataset with 0):\n",
      "\n",
      "                  Mean    Median  Standard Deviation  Count\n",
      "Method                                                     \n",
      "Approve       0.002129  0.002138            0.000364    140\n",
      "Execute       0.008488  0.007158            0.008664    159\n",
      "Other         0.008924  0.006515            0.011867    266\n",
      "Swap          0.008324  0.008476            0.003360     51\n",
      "Transfer      0.001807  0.001042            0.004412    944\n",
      "Unidentified  0.013133  0.008703            0.025296    123\n",
      "\n",
      "SUCCESS: Grouped Statistical Analysis conducted!\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Statistical Analysis\n",
    "\n",
    "# Calculate statistical data for the 'Value' column (including 0 values)\n",
    "Value_with0_stats = df[['Value']].agg(['mean', 'median', 'std', 'count']).transpose()\n",
    "\n",
    "# Calculate statistical data for the 'Value' column (excluding 0 values)\n",
    "Value_without0_stats = df_without_zeros[['Value']].agg(['mean', 'median', 'std', 'count']).transpose()\n",
    "\n",
    "# Calculate statistical data for the 'Txn_Fee' column\n",
    "Txn_Fee_stats = df[['Txn_Fee']].agg(['mean', 'median', 'std', 'count']).transpose()\n",
    "\n",
    "# Merge all statistical data into one dataframe\n",
    "merged_stats = pd.concat([Value_with0_stats, Value_without0_stats, Txn_Fee_stats], keys=['Full Dataset with 0:', 'Excluding 0 Values:', 'Full Dataset with 0:'])\n",
    "\n",
    "# Rename the column headers\n",
    "merged_stats.columns = ['Mean', 'Median', 'Standard Deviation', 'Count']\n",
    "\n",
    "print(\"----------------------------------------------------------------------------------------\")\n",
    "print(\"                            Ungrouped Statistical Analysis                              \")\n",
    "print(\"----------------------------------------------------------------------------------------\")\n",
    "print(merged_stats)\n",
    "\n",
    "print(\"\\nSUCCESS: Ungrouped Statistical Analysis conducted!\")\n",
    "\n",
    "# Group by 'Method' and calculate previous statistical calculations\n",
    "Group_Value_with0_stats = df.groupby('Method')[['Value']].agg(['mean', 'median', 'std', 'count'])\n",
    "Group_Value_without0_stats = df_without_zeros.groupby('Method')[['Value']].agg(['mean', 'median', 'std', 'count'])\n",
    "Group_Txn_Fee_stats = df.groupby('Method')[['Txn_Fee']].agg(['mean', 'median', 'std', 'count'])\n",
    "\n",
    "# Rename the column headers\n",
    "Group_Value_with0_stats.columns = ['Mean', 'Median', 'Standard Deviation', 'Count']\n",
    "Group_Value_without0_stats.columns = ['Mean', 'Median', 'Standard Deviation', 'Count']\n",
    "Group_Txn_Fee_stats.columns = ['Mean', 'Median', 'Standard Deviation', 'Count']\n",
    "\n",
    "print(\"----------------------------------------------------------------------------------------\")\n",
    "print(\"                             Grouped Statistical Analysis                               \")\n",
    "print(\"----------------------------------------------------------------------------------------\")\n",
    "print(\"--> Grouped analysis for Value (Full Dataset with 0):\\n\")\n",
    "print(Group_Value_with0_stats)\n",
    "print(\"----------------------------------------------------------------------------------------\")\n",
    "print(\"--> Grouped analysis for Value (Excluding 0 Values):\\n\")\n",
    "print(Group_Value_without0_stats)\n",
    "print(\"----------------------------------------------------------------------------------------\")\n",
    "print(\"--> Grouped analysis for Txn_Fee (Full Dataset with 0):\\n\")\n",
    "print(Group_Txn_Fee_stats)\n",
    "\n",
    "print(\"\\nSUCCESS: Grouped Statistical Analysis conducted!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-09T12:47:37.991744400Z",
     "start_time": "2024-03-09T12:47:37.962417600Z"
    }
   },
   "id": "137e79c0796696a5",
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6afea2a9797bc572"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Box plot for 'Value' and 'Txn_Fee' grouped by 'Method'\n",
    "# sns.boxplot(x='Method', y='Value', data=df, showfliers=False, palette='Blues')\n",
    "# sns.boxplot(x='Method', y='Txn_Fee', data=df, showfliers=False, palette='Oranges')\n",
    "# \n",
    "# plt.title('Box Plot of Value and Txn_Fee by Method')\n",
    "# plt.ylabel('Value')\n",
    "# plt.show()\n",
    "\n",
    "# # Create a figure and axis for the plots\n",
    "# fig, axs = plt.subplots(2, 1, figsize=(10, 8))\n",
    "# \n",
    "# # Plot histogram for 'Value' column\n",
    "# axs[0].hist(df['Value'], bins=100, color='skyblue', edgecolor='black')\n",
    "# axs[0].set_title('Histogram of Value')\n",
    "# axs[0].set_xlabel('Value')\n",
    "# axs[0].set_ylabel('Frequency')\n",
    "# \n",
    "# # Plot histogram for 'Txn fee' column\n",
    "# axs[1].hist(df['Txn_Fee'], bins=30, color='salmon', edgecolor='black')\n",
    "# axs[1].set_title('Histogram of Txn fee')\n",
    "# axs[1].set_xlabel('Txn_Fee')\n",
    "# axs[1].set_ylabel('Frequency')\n",
    "# \n",
    "# # Adjust layout\n",
    "# plt.tight_layout()\n",
    "# \n",
    "# # Display the histograms\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cfb5822943e3287"
  },
  {
   "cell_type": "markdown",
   "id": "87030e5e0b4fe1e6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## Data Sampling and Analysis\n",
    "\n",
    "In this section, we will delve into the process of data sampling and perform an initial analysis on the transaction data we have collected. Our objective is to understand the distribution of transaction values by sampling the data and comparing the sample statistics with the population statistics.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Load the Data**: Import the collected transaction data into a pandas DataFrame.\n",
    "\n",
    "2. **Data Cleaning**: Clean the data by handling missing values, converting data types, and removing any irrelevant information.\n",
    "\n",
    "3. **Simple Random Sampling (SRS)**: Create a sample from the dataset using a simple random sampling method. This involves randomly selecting a subset of the data without regard to any specific characteristics of the data.\n",
    "\n",
    "4. **Stratified Sampling**: Create another sample from the dataset using a stratified sampling method. This involves dividing the data into strata based on a specific characteristic (e.g., transaction value) and then randomly selecting samples from each stratum. Explain what you have stratified the data by and why you chose this column.\n",
    "\n",
    "5. **Statistical Analysis**: Calculate the mean and standard deviation of the samples and the population. Compare these statistics to understand the distribution of transaction values.\n",
    "\n",
    "6. **Visualization**: Plot the distribution of transaction values and fees for both the samples and the population to visually compare their distributions.\n",
    "\n",
    "### Considerations\n",
    "\n",
    "- **Sample Size**: The size of the sample should be large enough to represent the population accurately but not so large that it becomes impractical to analyze.\n",
    "- **Sampling Method**: Choose the appropriate sampling method based on the characteristics of the data and the research question.\n",
    "\n",
    "Explain the above considerations in your report."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
